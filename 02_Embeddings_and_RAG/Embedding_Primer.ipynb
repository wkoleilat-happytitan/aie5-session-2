{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Dense Vector Search: A Quick Primer\n",
    "\n",
    "If you come from an NLP background, embeddings are something you might be intimately familiar with - otherwise, you might find the topic a bit...dense. (this attempt at a joke will make more sense later)\n",
    "\n",
    "In all seriousness, embeddings are a powerful piece of the NLP puzzle, so let's dive in!\n",
    "\n",
    "> NOTE: While this notebook language/NLP-centric, embeddings have uses beyond just text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Do We Even Need Embeddings?\n",
    "\n",
    "In order to fully understand what Embeddings are, we first need to understand why we have them!\n",
    "\n",
    "Machine Learning algorithms, ranging from the very big to the very small, all have one thing in common:\n",
    "\n",
    "They need numeric inputs.\n",
    "\n",
    "So we need a process by which to translate the domain we live in, dominated by images, audio, language, and more, into the domain of the machine: Numbers.\n",
    "\n",
    "Another thing we want to be able to do is capture \"semantic information\" about words/phrases so that we can use algorithmic approaches to determine if words are closely related or not!\n",
    "\n",
    "So, we need to come up with a process that does these two things well:\n",
    "\n",
    "- Convert non-numeric data into numeric-data\n",
    "- Capture potential semantic relationships between individual pieces of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Do Embeddings Capture Semantic Relationships?\n",
    "\n",
    "In a simplified sense, embeddings map a word or phrase into n-dimensional space with a dense continuous vector, where each dimension in the vector represents some \"latent feature\" of the data.\n",
    "\n",
    "This is best represented in a classic example:\n",
    "\n",
    "![image](https://i.imgur.com/K5eQtmH.png)\n",
    "\n",
    "As can be seen in the extremely simplified example: The X_1 axis represents age, and the X_2 axis represents hair.\n",
    "\n",
    "The relationship of \"puppy -> dog\" reflects the same relationship as \"baby -> adult\", but dogs are (typically) hairier than humans. However, adults typically have more hair than babies - so they are shifted slightly closer to dogs on the X_2 axis!\n",
    "\n",
    "Now, this is a simplified and contrived example - but it is *essentially* the mechanism by which embeddings capture semantic information.\n",
    "\n",
    "In reality, the dimensions don't sincerely represent hard-concepts like \"age\" or \"hair\", but it's useful as a way to think about how the semantic relationships are captured.\n",
    "\n",
    "Alright, with some history behind us - let's examine how these might help us choose relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple example - simply looking at how close to embedding vectors are for a given phrase.\n",
    "\n",
    "When we use the term \"close\" in this notebook - we're referring to a distance measure called \"cosine similarity\".\n",
    "\n",
    "We discussed above that if two embeddings are close - they are semantically similar, cosine similarity gives us a quick way to measure how similar two vectors are!\n",
    "\n",
    "Closeness is measured from 1 to -1, with 1 being extremely close and -1 being extremely close to opposite in meaning.\n",
    "\n",
    "Let's implement it with Numpy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `text-embedding-3-small` embedding model (more on that in a second) to embed two sentences. In order to use this embedding model endpoint - we'll need to provide our OpenAI API key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
    "\n",
    "embedding_model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "puppy_sentence = \"I love puppies!\"\n",
    "dog_sentence = \"I love dogs!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert those into embedding vectors using OpenAI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
    "dog_vector = embedding_model.get_embedding(dog_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can determine how closely they are related using our distance measure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8341482011091341)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(puppy_vector, dog_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, with cosine similarity, close to 1. means they're very close!\n",
    "\n",
    "Let's see what happens if we use a different set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3723972998892517)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puppy_sentence = \"I love puppies!\"\n",
    "cat_sentence = \"I dislike cats!\"\n",
    "\n",
    "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
    "cat_vector = embedding_model.get_embedding(cat_sentence)\n",
    "\n",
    "cosine_similarity(puppy_vector, cat_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - these vectors are further apart - as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Vector Calculations\n",
    "\n",
    "One of the ways that Embedding Vectors can be leveraged, and a fun \"proof\" that they work the way we expected can be explored via \"Vector Calculations\"\n",
    "\n",
    "That is to say: If we take the vector for \"King\", and subtract the vector for \"man\", and add the vector for \"woman\" - we should have a vector that is similar to \"Queen\".\n",
    "\n",
    "Let's try this out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7161951721027584)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king_vector = np.array(embedding_model.get_embedding(\"King\"))\n",
    "man_vector = np.array(embedding_model.get_embedding(\"man\"))\n",
    "woman_vector = np.array(embedding_model.get_embedding(\"woman\"))\n",
    "\n",
    "vector_calculation_result = king_vector - man_vector + woman_vector\n",
    "\n",
    "queen_vector = np.array(embedding_model.get_embedding(\"Queen\"))\n",
    "\n",
    "cosine_similarity(vector_calculation_result, queen_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Walid] I am going to try this same exercise and see the cosine similarity between a sentence in french and the same sentence in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8134808594458636)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_speak_french_eng = np.array(embedding_model.get_embedding(\"I speak french\"))\n",
    "i_speak_french_fr = np.array(embedding_model.get_embedding(\"Je parle franÃ§ais\"))\n",
    "\n",
    "cosine_similarity(i_speak_french_eng, i_speak_french_fr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - the resulting vector is indeed quite close to the \"Queen\" vector!\n",
    "\n",
    "> NOTE: The loss is explained by the vectors not *literally* encoding information along axes as simple as \"man\" or \"woman\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Walid] I am going to try and generate the embeddings of two images and calculate their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    # Open and prepare the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Create embedding using OpenAI's GPT-4 Vision\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{encode_image(image)}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    # Get the description and convert it to embedding\n",
    "    description = response.choices[0].message.content\n",
    "    # Convert the text description to an embedding using our existing embedding model\n",
    "    return np.array(embedding_model.get_embedding(description))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image):\n",
    "    import base64\n",
    "    import io\n",
    "    \n",
    "    # Convert PIL Image to bytes\n",
    "    buffered = io.BytesIO()\n",
    "    # Convert image to RGB mode if it's not already\n",
    "    if image.mode in ('RGBA', 'P'):\n",
    "        image = image.convert('RGB')\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    \n",
    "    # Encode to base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two images\n",
    "def compare_images(image_path1, image_path2):\n",
    "    # Get embeddings for both images\n",
    "    embedding1 = get_image_embedding(image_path1)\n",
    "    embedding2 = get_image_embedding(image_path2)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am calculating the cosine similarity between two poodle images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = \"images/poodle1.jpg\"\n",
    "image2_path = \"images/poodle2.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between images: 0.6114003143213\n"
     ]
    }
   ],
   "source": [
    "similarity_score = compare_images(image1_path, image2_path)\n",
    "print(f\"Similarity score between images: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am calculating the cosine similarity between a poodle and a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = \"images/poodle1.jpg\"\n",
    "image2_path = \"images/cat.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between images: 0.28740342245942885\n"
     ]
    }
   ],
   "source": [
    "similarity_score = compare_images(image1_path, image2_path)\n",
    "print(f\"Similarity score between images: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am calculating the cosine similarity between a poodle and german sheperd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = \"images/poodle1.jpg\"\n",
    "image2_path = \"images/germanshepherd.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between images: 0.36598593994155265\n"
     ]
    }
   ],
   "source": [
    "similarity_score = compare_images(image1_path, image2_path)\n",
    "print(f\"Similarity score between images: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As you can see - embeddings can help us convert text into a machine understandable format, which we can leverage for a number of purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
